{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a244afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87093221",
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_data = json.load(open('sal.json'))\n",
    "raw_suburbs = sal_data.keys()\n",
    "\n",
    "clean_suburbs = []\n",
    "suburb_extra_info_1 = []\n",
    "suburb_extra_info_2 = []\n",
    "suburb_gcc = []\n",
    "states = []\n",
    "\n",
    "state_names = {\n",
    " 'nsw': 'new south wales',\n",
    " 'vic': 'victoria',\n",
    " 'qld': 'queensland',\n",
    " 'tas': 'tasmania',\n",
    " 'wa': 'western australia',\n",
    " 'sa': 'south australia',\n",
    " 'act': 'australian capital terriroty',\n",
    " 'wa': 'western australia',\n",
    "   'nt': 'northern territory' \n",
    "}\n",
    "\n",
    "for suburb in raw_suburbs:\n",
    "    gcc = sal_data[suburb]['gcc']\n",
    "    suburb_gcc.append(gcc)\n",
    "\n",
    "    #check whether there is any additional info in brackets\n",
    "    extra_info = re.search(\"\\([\\w\\-\\ .]+\\)\", suburb)\n",
    "\n",
    "    if extra_info != None:\n",
    "\n",
    "        #if there is additional info in brackets, check whether it is region info\n",
    "\n",
    "        suburb = suburb.replace(extra_info.group(0),'').strip() #clean suburb name\n",
    "        clean_suburbs.append(suburb)\n",
    "        extra_info_2 = re.search(\"[\\w\\ ]+\\ \\-\", extra_info.group(0))\n",
    "\n",
    "        if extra_info_2 != None:         \n",
    "            #if there is region info, add it to a 2nd info column            \n",
    "            suburb_extra_info_2.append(extra_info_2.group(0).replace('-','').strip())\n",
    "            extra_info_1 = re.search(\"\\-\\ \\w+\", extra_info.group(0))\n",
    "            suburb_extra_info_1.append(extra_info_1.group(0).replace('-','').replace('.','').strip())\n",
    "\n",
    "        else:\n",
    "            suburb_extra_info_2.append(None)\n",
    "            suburb_extra_info_1.append(extra_info.group(0).replace(')','').replace('(','').replace('.',''))\n",
    "\n",
    "\n",
    "    #if no state/region info - append suburb as is (no cleaning) & no additional info\n",
    "    else:\n",
    "        suburb_extra_info_1.append(None)\n",
    "        suburb_extra_info_2.append(None)\n",
    "        clean_suburbs.append(suburb)\n",
    "\n",
    "suburbs_df = pd.DataFrame(\n",
    "{'raw_suburb': raw_suburbs,\n",
    "'gcc': suburb_gcc,\n",
    "#'state': states,\n",
    "'clean_suburb': clean_suburbs,\n",
    "'info_1': suburb_extra_info_1,\n",
    "'info_2': suburb_extra_info_2}\n",
    ")\n",
    "\n",
    "suburbs_df = suburbs_df[~suburbs_df['raw_suburb'].isin(['belconnen (act)',\n",
    "                                                        'canberra (act)',\n",
    "                                                        'gungahlin (act)',\n",
    "                                                        'hall (act)',\n",
    "                                                        'perth'])]\n",
    "\n",
    "#find clean suburbs that have more than 1 location\n",
    "agg_df = suburbs_df.groupby('clean_suburb').count().reset_index()\n",
    "agg_df.rename(columns={'raw_suburb':'suburb_name_instance_count'}, inplace=True)\n",
    "\n",
    "suburbs_df = pd.merge(suburbs_df, agg_df[['clean_suburb', 'suburb_name_instance_count']], on='clean_suburb', how='left')\n",
    "suburbs_df.loc[suburbs_df['raw_suburb']=='jerrabomberra', 'info_1'] = 'nsw'\n",
    "suburbs_df.loc[suburbs_df['raw_suburb']=='coree', 'info_1'] = 'nsw'\n",
    "\n",
    "\n",
    "suburbs_df['state'] = suburbs_df['info_1'].map(state_names)\n",
    "\n",
    "unique_suburb_dict = suburbs_df[suburbs_df['suburb_name_instance_count']==1][['clean_suburb','gcc']].set_index('clean_suburb').to_dict()['gcc']\n",
    "region_names_dict = suburbs_df[suburbs_df['info_2'].notnull()][['info_2','gcc']].set_index('info_2').to_dict()['gcc']\n",
    "non_unique_with_state = suburbs_df[(suburbs_df['info_1'].notnull())&(suburbs_df['suburb_name_instance_count']!=1)][['clean_suburb','state','gcc']].set_index(['clean_suburb', 'state']).to_dict()['gcc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "326a3bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_file = \"C:/temp/Twitter2023/twitter-huge/mnt/ext100/twitter-huge.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ace5a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_map = {'interest rate': 'interest rate', \n",
    "             ' rba': 'interest rate',\n",
    "             \"rba decision\": 'interest rate', \n",
    "             r\"rba's decision\": 'interest rate',\n",
    "             'cash rate': 'interest rate', \n",
    "             'interest payment': 'interest rate', \n",
    "             'interest repayment': 'interest rate', \n",
    "             'interest re-payment': 'interest rate',\n",
    "             'repayment of interest': 'interest rate', \n",
    "             'variable interest': 'interest rate',\n",
    "             'fixed interest': 'interest rate',\n",
    "             'bank interest': 'interest rate',\n",
    "             'rate hike': 'interest rate', \n",
    "             'mortgage': 'housing', \n",
    "             'rent payment': 'housing', \n",
    "             'house rent': 'housing', \n",
    "             'houserent': 'housing', \n",
    "             'house payment': 'housing',\n",
    "             'housing': 'housing',\n",
    "             'inflation': 'inflation', \n",
    "             'cpi index': 'inflation',\n",
    "             'cost of living': 'inflation',\n",
    "             'shrinkflation': 'inflation',\n",
    " #            'investment': 'finances',\n",
    " #            'money': 'finances',\n",
    " #            'cash': 'finances', \n",
    "             'social security':'social security', \n",
    "             'job seeker':'social security',\n",
    "             'jobseeker':'social security',\n",
    "             'youth allowance':'social security',\n",
    "             'austudy':'social security',\n",
    "             'centrelink':'social security', \n",
    "             'centerlink':'social security'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc5bb8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1683355357.0763407\n",
      "dict_keys(['interest rate', ' rba', 'rba decision', \"rba's decision\", 'cash rate', 'interest payment', 'interest repayment', 'interest re-payment', 'repayment of interest', 'variable interest', 'fixed interest', 'bank interest', 'rate hike', 'mortgage', 'rent payment', 'house rent', 'houserent', 'house payment', 'housing', 'inflation', 'cpi index', 'cost of living', 'shrinkflation', 'social security', 'job seeker', 'jobseeker', 'youth allowance', 'austudy', 'centrelink', 'centerlink'])\n",
      "Number of search tearms : 30\n",
      "The job took 643.17 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "# Sampling tweets without ijson\n",
    "\n",
    "f = open(twitter_file, encoding=\"utf8\")\n",
    "\n",
    "start_time = time.time()\n",
    "print(start_time)\n",
    "\n",
    "kwd_list = topic_map.keys()\n",
    "print(kwd_list)\n",
    "print(\"Number of search tearms :\", len(kwd_list))\n",
    "\n",
    "timestamp_data = []\n",
    "text_data = []\n",
    "location_data = []\n",
    "gcc_data = []\n",
    "# state_data = []\n",
    "search_term_data = []\n",
    "tweet_ids_data = []\n",
    "sentiment_data = []\n",
    "author_id_data = []\n",
    "coordinates_data = []\n",
    "\n",
    "for line in f:\n",
    "    text_re = re.search(r'},\"text\":\"(.*)\",\"sentiment\"', line)\n",
    "    if text_re != None:\n",
    "        text = text_re.group(1).lower()\n",
    "        for kwd in kwd_list:\n",
    "            if kwd in text:\n",
    "                tweet = json.loads(line.replace(',\\n',''))\n",
    "\n",
    "                tweet_ids_data.append(tweet[\"id\"])\n",
    "                author_id_data.append(tweet['doc']['data']['author_id'])\n",
    "                timestamp_data.append(tweet['doc']['data']['created_at'])\n",
    "                text_data.append(tweet['doc']['data']['text'])\n",
    "                search_term_data.append(kwd)  \n",
    "                sentiment_data.append(tweet['doc']['data']['sentiment'])\n",
    "                \n",
    "                try:\n",
    "                    coordinates = tweet['doc']['data']['geo']['coordinates']['coordinates']\n",
    "                    coordinates_data.append(coordinates)\n",
    "                except:\n",
    "                    coordinates_data.append(None)\n",
    "                \n",
    "                try:\n",
    "                    location = tweet['doc']['includes']['places'][0]['full_name']\n",
    "                    location_data.append(location)\n",
    "                    location_split = re.split(',',location.lower())  \n",
    "\n",
    "                    if location_split[0] in unique_suburb_dict:\n",
    "                        gcc_data.append(unique_suburb_dict[location_split[0]])\n",
    "\n",
    "                    elif location_split[0] in region_names_dict:\n",
    "                        gcc_data.append(region_names_dict[location_split[0]])\n",
    "\n",
    "                    #suburb name not unique\n",
    "                    elif len(location_split) == 2: #only 2 fields\n",
    "                        if (location_split[0], location_split[1].strip()) in non_unique_with_state:\n",
    "                            gcc_data.append(non_unique_with_state[(location_split[0], location_split[1].strip())])\n",
    "                        else:\n",
    "                            gcc_data.append(None)\n",
    "\n",
    "\n",
    "                    elif len(location_split) == 3: #3 fields\n",
    "                        if (location_split[0], location_split[2].strip()) in non_unique_with_state:\n",
    "                            gcc_data.append(non_unique_with_state[(location_split[0], location_split[2].strip())])\n",
    "                        else:\n",
    "                            gcc_data.append(None)\n",
    "                    else:\n",
    "                        gcc_data.append(None)           \n",
    "                                      \n",
    "                except: \n",
    "                    location_data.append(None)\n",
    "                    gcc_data.append(None)\n",
    "\n",
    "\n",
    "    \n",
    "end_time = time.time()\n",
    "print(f\"The job took {round(end_time - start_time, 3)} seconds to complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78395268",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'id':tweet_ids_data,\n",
    "    'author_id': author_id_data,\n",
    "    'timestamp':timestamp_data,\n",
    "    'text':text_data,\n",
    "    'search_term':search_term_data,\n",
    "    'location':location_data,\n",
    "    'coordinates':coordinates_data,\n",
    "    'sentiment' : sentiment_data,\n",
    "    'gcc' : gcc_data \n",
    "})\n",
    "\n",
    "\n",
    "df['topic'] = df['search_term'].map(\n",
    "topic_map\n",
    ")\n",
    "\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], format=\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "df['year'] = df['timestamp'].dt.year\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['day'] = df['timestamp'].dt.day\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['week'] = df['timestamp'].dt.isocalendar().week\n",
    "\n",
    "df['state'] = df['location'].map(\n",
    "{'Victoria, Australia': 'vic',\n",
    "'New South Wales, Australia': 'nsw',\n",
    "'Western Australia, Australia': 'wa',\n",
    " 'South Australia, Australia': 'sa',\n",
    " 'Northern Territory, Australia': 'nt',\n",
    " 'Tasmania, Australia': 'tas',\n",
    " 'Australian Capital Territory, Australia': 'act',\n",
    " 'Queensland, Australia': 'qld',\n",
    " 'Victoria, Australia': 'vic',\n",
    "})\n",
    "\n",
    "df['state'] = np.where(df.state.isna(), df['gcc'].map(\n",
    "{\n",
    "    '1gsyd':'nsw',\n",
    "    '1rnsw': 'nsw',\n",
    "    '2gmel':'vic',\n",
    "    '2rvic': 'vic',\n",
    "    '3gbri': 'qld',\n",
    "    '3rqld': 'qld',\n",
    "    '4gade': 'sa',\n",
    "    '4rsau': 'sa',\n",
    "    '5gper': 'wa',\n",
    "    '5rwau': 'wa',\n",
    "    '6ghob': 'tas',\n",
    "    '6rtas': 'tas',\n",
    "    '7gdar': 'nt',\n",
    "    '7rnte': 'nt',\n",
    "    '8acte': 'act'\n",
    "}), df.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36356fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gcc</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2gmel</td>\n",
       "      <td>2099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1gsyd</td>\n",
       "      <td>1815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3gbri</td>\n",
       "      <td>1062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3rqld</td>\n",
       "      <td>764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4gade</td>\n",
       "      <td>601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5gper</td>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1rnsw</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6ghob</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8acte</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2rvic</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5rwau</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7gdar</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6rtas</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4rsau</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7rnte</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      gcc    id\n",
       "0   2gmel  2099\n",
       "1   1gsyd  1815\n",
       "2   3gbri  1062\n",
       "3   3rqld   764\n",
       "4   4gade   601\n",
       "5   5gper   506\n",
       "6   1rnsw   438\n",
       "7   6ghob   414\n",
       "8   8acte   285\n",
       "9   2rvic   273\n",
       "10  5rwau    92\n",
       "11  7gdar    83\n",
       "12  6rtas    72\n",
       "13  4rsau    65\n",
       "14  7rnte    31"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('gcc').agg({'id':'nunique'}).sort_values('id', ascending=False).head(20).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "265e6aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174259"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['location'].isna()]['id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed577cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9645"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[~df['location'].isna()]['id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d816fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "df = df.drop_duplicates(subset='id', keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f059b744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as csv\n",
    "df.to_csv('twitter.csv', index=False)\n",
    "df[~df['location'].isna()].to_csv('twitter_small.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "355225ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as json\n",
    "df.to_json(r'twitter.json', orient='records')\n",
    "with open('twitter.json', 'r', encoding=\"utf8\") as f:\n",
    "    data = json.load(f)\n",
    "data_dict = {}\n",
    "data_dict['docs'] = data\n",
    "with open('twitter.json', 'w', encoding=\"utf8\") as f:\n",
    "    json.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9544e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1986eb0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
