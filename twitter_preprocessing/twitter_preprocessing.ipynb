{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a244afe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87093221",
   "metadata": {},
   "outputs": [],
   "source": [
    "sal_data = json.load(open('sal.json'))\n",
    "raw_suburbs = sal_data.keys()\n",
    "\n",
    "clean_suburbs = []\n",
    "suburb_extra_info_1 = []\n",
    "suburb_extra_info_2 = []\n",
    "suburb_gcc = []\n",
    "states = []\n",
    "\n",
    "state_names = {\n",
    " 'nsw': 'new south wales',\n",
    " 'vic': 'victoria',\n",
    " 'qld': 'queensland',\n",
    " 'tas': 'tasmania',\n",
    " 'wa': 'western australia',\n",
    " 'sa': 'south australia',\n",
    " 'act': 'australian capital terriroty',\n",
    " 'wa': 'western australia',\n",
    "   'nt': 'northern territory' \n",
    "}\n",
    "\n",
    "for suburb in raw_suburbs:\n",
    "    gcc = sal_data[suburb]['gcc']\n",
    "    suburb_gcc.append(gcc)\n",
    "\n",
    "    #check whether there is any additional info in brackets\n",
    "    extra_info = re.search(\"\\([\\w\\-\\ .]+\\)\", suburb)\n",
    "\n",
    "    if extra_info != None:\n",
    "\n",
    "        #if there is additional info in brackets, check whether it is region info\n",
    "\n",
    "        suburb = suburb.replace(extra_info.group(0),'').strip() #clean suburb name\n",
    "        clean_suburbs.append(suburb)\n",
    "        extra_info_2 = re.search(\"[\\w\\ ]+\\ \\-\", extra_info.group(0))\n",
    "\n",
    "        if extra_info_2 != None:         \n",
    "            #if there is region info, add it to a 2nd info column            \n",
    "            suburb_extra_info_2.append(extra_info_2.group(0).replace('-','').strip())\n",
    "            extra_info_1 = re.search(\"\\-\\ \\w+\", extra_info.group(0))\n",
    "            suburb_extra_info_1.append(extra_info_1.group(0).replace('-','').replace('.','').strip())\n",
    "\n",
    "        else:\n",
    "            suburb_extra_info_2.append(None)\n",
    "            suburb_extra_info_1.append(extra_info.group(0).replace(')','').replace('(','').replace('.',''))\n",
    "\n",
    "\n",
    "    #if no state/region info - append suburb as is (no cleaning) & no additional info\n",
    "    else:\n",
    "        suburb_extra_info_1.append(None)\n",
    "        suburb_extra_info_2.append(None)\n",
    "        clean_suburbs.append(suburb)\n",
    "\n",
    "suburbs_df = pd.DataFrame(\n",
    "{'raw_suburb': raw_suburbs,\n",
    "'gcc': suburb_gcc,\n",
    "#'state': states,\n",
    "'clean_suburb': clean_suburbs,\n",
    "'info_1': suburb_extra_info_1,\n",
    "'info_2': suburb_extra_info_2}\n",
    ")\n",
    "\n",
    "suburbs_df = suburbs_df[~suburbs_df['raw_suburb'].isin(['belconnen (act)',\n",
    "                                                        'canberra (act)',\n",
    "                                                        'gungahlin (act)',\n",
    "                                                        'hall (act)',\n",
    "                                                        'perth'])]\n",
    "\n",
    "#find clean suburbs that have more than 1 location\n",
    "agg_df = suburbs_df.groupby('clean_suburb').count().reset_index()\n",
    "agg_df.rename(columns={'raw_suburb':'suburb_name_instance_count'}, inplace=True)\n",
    "\n",
    "suburbs_df = pd.merge(suburbs_df, agg_df[['clean_suburb', 'suburb_name_instance_count']], on='clean_suburb', how='left')\n",
    "suburbs_df.loc[suburbs_df['raw_suburb']=='jerrabomberra', 'info_1'] = 'nsw'\n",
    "suburbs_df.loc[suburbs_df['raw_suburb']=='coree', 'info_1'] = 'nsw'\n",
    "\n",
    "\n",
    "suburbs_df['state'] = suburbs_df['info_1'].map(state_names)\n",
    "\n",
    "unique_suburb_dict = suburbs_df[suburbs_df['suburb_name_instance_count']==1][['clean_suburb','gcc']].set_index('clean_suburb').to_dict()['gcc']\n",
    "region_names_dict = suburbs_df[suburbs_df['info_2'].notnull()][['info_2','gcc']].set_index('info_2').to_dict()['gcc']\n",
    "non_unique_with_state = suburbs_df[(suburbs_df['info_1'].notnull())&(suburbs_df['suburb_name_instance_count']!=1)][['clean_suburb','state','gcc']].set_index(['clean_suburb', 'state']).to_dict()['gcc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326a3bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_file = \"twitter-huge.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ace5a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_map = {'interest rate': 'interest rate', \n",
    "             ' rba': 'interest rate',\n",
    "             \"rba decision\": 'interest rate', \n",
    "             r\"rba's decision\": 'interest rate',\n",
    "             'cash rate': 'interest rate', \n",
    "             'interest payment': 'interest rate', \n",
    "             'interest repayment': 'interest rate', \n",
    "             'interest re-payment': 'interest rate',\n",
    "             'repayment of interest': 'interest rate', \n",
    "             'variable interest': 'interest rate',\n",
    "             'fixed interest': 'interest rate',\n",
    "             'bank interest': 'interest rate',\n",
    "             'rate hike': 'interest rate', \n",
    "             'mortgage': 'housing', \n",
    "             'rent payment': 'housing', \n",
    "             'house rent': 'housing', \n",
    "             'houserent': 'housing', \n",
    "             'house payment': 'housing',\n",
    "             'housing': 'housing',\n",
    "             'inflation': 'inflation', \n",
    "             'cpi index': 'inflation',\n",
    "             'cost of living': 'inflation',\n",
    "             'shrinkflation': 'inflation',\n",
    "             'social security':'social security', \n",
    "             'job seeker':'social security',\n",
    "             'jobseeker':'social security',\n",
    "             'youth allowance':'social security',\n",
    "             'austudy':'social security',\n",
    "             'centrelink':'social security', \n",
    "             'centerlink':'social security'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5bb8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(twitter_file, encoding=\"utf8\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "kwd_list = topic_map.keys()\n",
    "print(kwd_list)\n",
    "print(\"Number of search tearms :\", len(kwd_list))\n",
    "\n",
    "timestamp_data = []\n",
    "text_data = []\n",
    "location_data = []\n",
    "gcc_data = []\n",
    "# state_data = []\n",
    "search_term_data = []\n",
    "tweet_ids_data = []\n",
    "sentiment_data = []\n",
    "author_id_data = []\n",
    "coordinates_data = []\n",
    "\n",
    "for line in f:\n",
    "    text_re = re.search(r'},\"text\":\"(.*)\",\"sentiment\"', line)\n",
    "    if text_re != None:\n",
    "        text = text_re.group(1).lower()\n",
    "        for kwd in kwd_list:\n",
    "            if kwd in text:\n",
    "                tweet = json.loads(line.replace(',\\n',''))\n",
    "\n",
    "                tweet_ids_data.append(tweet[\"id\"])\n",
    "                author_id_data.append(tweet['doc']['data']['author_id'])\n",
    "                timestamp_data.append(tweet['doc']['data']['created_at'])\n",
    "                text_data.append(tweet['doc']['data']['text'])\n",
    "                search_term_data.append(kwd)  \n",
    "                sentiment_data.append(tweet['doc']['data']['sentiment'])\n",
    "                \n",
    "                try:\n",
    "                    coordinates = tweet['doc']['data']['geo']['coordinates']['coordinates']\n",
    "                    coordinates_data.append(coordinates)\n",
    "                except:\n",
    "                    coordinates_data.append(None)\n",
    "                \n",
    "                try:\n",
    "                    location = tweet['doc']['includes']['places'][0]['full_name']\n",
    "                    location_data.append(location)\n",
    "                    location_split = re.split(',',location.lower())  \n",
    "\n",
    "                    if location_split[0] in unique_suburb_dict:\n",
    "                        gcc_data.append(unique_suburb_dict[location_split[0]])\n",
    "\n",
    "                    elif location_split[0] in region_names_dict:\n",
    "                        gcc_data.append(region_names_dict[location_split[0]])\n",
    "\n",
    "                    #suburb name not unique\n",
    "                    elif len(location_split) == 2: #only 2 fields\n",
    "                        if (location_split[0], location_split[1].strip()) in non_unique_with_state:\n",
    "                            gcc_data.append(non_unique_with_state[(location_split[0], location_split[1].strip())])\n",
    "                        else:\n",
    "                            gcc_data.append(None)\n",
    "\n",
    "\n",
    "                    elif len(location_split) == 3: #3 fields\n",
    "                        if (location_split[0], location_split[2].strip()) in non_unique_with_state:\n",
    "                            gcc_data.append(non_unique_with_state[(location_split[0], location_split[2].strip())])\n",
    "                        else:\n",
    "                            gcc_data.append(None)\n",
    "                    else:\n",
    "                        gcc_data.append(None)           \n",
    "                                      \n",
    "                except: \n",
    "                    location_data.append(None)\n",
    "                    gcc_data.append(None)\n",
    "\n",
    "\n",
    "    \n",
    "end_time = time.time()\n",
    "print(f\"The job took {round(end_time - start_time, 3)} seconds to complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78395268",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'id':tweet_ids_data,\n",
    "    'author_id': author_id_data,\n",
    "    'timestamp':timestamp_data,\n",
    "    'text':text_data,\n",
    "    'search_term':search_term_data,\n",
    "    'location':location_data,\n",
    "    'coordinates':coordinates_data,\n",
    "    'sentiment' : sentiment_data,\n",
    "    'gcc' : gcc_data \n",
    "})\n",
    "\n",
    "\n",
    "df['topic'] = df['search_term'].map(\n",
    "topic_map\n",
    ")\n",
    "\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], format=\"%Y-%m-%dT%H:%M:%S.000Z\")\n",
    "df['year'] = df['timestamp'].dt.year\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['day'] = df['timestamp'].dt.day\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['week'] = df['timestamp'].dt.isocalendar().week\n",
    "\n",
    "df['state'] = df['location'].map(\n",
    "{'Victoria, Australia': 'vic',\n",
    "'New South Wales, Australia': 'nsw',\n",
    "'Western Australia, Australia': 'wa',\n",
    " 'South Australia, Australia': 'sa',\n",
    " 'Northern Territory, Australia': 'nt',\n",
    " 'Tasmania, Australia': 'tas',\n",
    " 'Australian Capital Territory, Australia': 'act',\n",
    " 'Queensland, Australia': 'qld',\n",
    " 'Victoria, Australia': 'vic',\n",
    "})\n",
    "\n",
    "df['state'] = np.where(df.state.isna(), df['gcc'].map(\n",
    "{\n",
    "    '1gsyd':'nsw',\n",
    "    '1rnsw': 'nsw',\n",
    "    '2gmel':'vic',\n",
    "    '2rvic': 'vic',\n",
    "    '3gbri': 'qld',\n",
    "    '3rqld': 'qld',\n",
    "    '4gade': 'sa',\n",
    "    '4rsau': 'sa',\n",
    "    '5gper': 'wa',\n",
    "    '5rwau': 'wa',\n",
    "    '6ghob': 'tas',\n",
    "    '6rtas': 'tas',\n",
    "    '7gdar': 'nt',\n",
    "    '7rnte': 'nt',\n",
    "    '8acte': 'act'\n",
    "}), df.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d816fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "df = df.drop_duplicates(subset='id', keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355225ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save as json\n",
    "df.to_json(r'twitter.json', orient='records')\n",
    "with open('twitter.json', 'r', encoding=\"utf8\") as f:\n",
    "    data = json.load(f)\n",
    "data_dict = {}\n",
    "data_dict['docs'] = data\n",
    "with open('twitter.json', 'w', encoding=\"utf8\") as f:\n",
    "    json.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9544e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1986eb0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
